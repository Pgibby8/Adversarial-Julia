{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Learning on Malware Data\n",
    "Data derived from: https://figshare.com/articles/Android_malware_dataset_for_machine_learning_1/5854590/1\n",
    "\n",
    "The first few cells are dedicated to processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "process_binary_data (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using Statistics\n",
    "using Knet: Knet, KnetArray, gpu, minibatch\n",
    "using StatsBase\n",
    "import Missings\n",
    "using Optim\n",
    "using Knet: SGD, train!, nll, zeroone, relu, dropout\n",
    "using AutoGrad: Param\n",
    "import ProgressMeter\n",
    "\n",
    "function remove_missing!(df)\n",
    "    for col in names(df)\n",
    "        df[col] = Missings.coalesce.(df[col], 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "function process_binary_data(data, train_rat)\n",
    "    \"\"\"\n",
    "    Takes in data in the form of a DataFrame with a :class\n",
    "    column containing either 1 or 2 (binary classification)\n",
    "    Returns a training set, a testing set, and all entries\n",
    "    in class 2 (for use in permuting them adversarially)\n",
    "    \"\"\"\n",
    "    suspicious = data[data[:class] .== 2, :]\n",
    "    benign = data[data[:class] .== 1, :]\n",
    "\n",
    "    train_benign_count = Int(round(size(benign)[1]*train_rat))\n",
    "    train_susp_count = Int(round(size(suspicious)[1]*train_rat))\n",
    "\n",
    "    s_train_inds = sample(1:size(suspicious)[1], train_susp_count, replace=false)\n",
    "    b_train_inds = sample(1:size(benign)[1], train_benign_count, replace=false)\n",
    "    s_test_inds = setdiff(1:size(suspicious)[1], s_train_inds)\n",
    "    b_test_inds = setdiff(1:size(benign)[1], b_train_inds)\n",
    "\n",
    "    s_train = suspicious[s_train_inds, :]\n",
    "    s_test = suspicious[s_test_inds, :]\n",
    "    b_train = benign[b_train_inds, :]\n",
    "    b_test = benign[b_test_inds, :]\n",
    "\n",
    "    trn = [s_train; b_train]\n",
    "    remove_missing!(trn)\n",
    "    tst = [s_test; b_test]\n",
    "    remove_missing!(tst)\n",
    "\n",
    "    shuffle = sample(1:size(trn)[1], size(trn)[1], replace=false)\n",
    "    dtrn = Matrix(trn[shuffle, :])'\n",
    "    trnx, trny = dtrn[1:end - 1, :], dtrn[end, :]\n",
    "    # trny = hcat(trny, -trny .+ 1)'\n",
    "    dtrn = minibatch(trnx, trny, 100)\n",
    "    shuffle = sample(1:size(tst)[1], size(tst)[1], replace=false)\n",
    "    dtst = Matrix(tst[shuffle, :])'\n",
    "    tstx, tsty = dtst[1:end - 1, :], dtst[end, :]\n",
    "    # tsty = hcat(tsty, -tsty .+ 1)'\n",
    "    dtst = minibatch(tstx, tsty, 100)\n",
    "\n",
    "    remove_missing!(suspicious)\n",
    "    susp = Matrix(suspicious)'[1:end-1, :]\n",
    "    return dtrn, dtst, susp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Knet.Data([1 0 … 1 1; 1 0 … 1 1; … ; 0 0 … 0 0; 0 0 … 0 0], [1 1 … 1 1], 100, 2659, false, 1:2659, false, (215, 2659), (2659,), Array{Int64,2}, Array{Int64,1}), Knet.Data([1 0 … 0 0; 0 0 … 1 0; … ; 0 0 … 0 0; 0 0 … 0 0], [1 2 … 1 2], 100, 1140, false, 1:1140, false, (215, 1140), (1140,), Array{Int64,2}, Array{Int64,1}), [0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"malgenome-215-dataset-1260malware-2539-benign.csv\")\n",
    "bin = Dict(\"B\" => 1, \"S\" => 2)\n",
    "data[:class] = map(elt->bin[elt], data[:class])\n",
    "\n",
    "dtrn, dtst, susp = process_binary_data(data, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining an MLP to attack\n",
    "\n",
    "Code primarily derived from the Knet tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(x::Array{Float32, 1})\n",
    "    ret = exp.(x)\n",
    "    ret / sum(ret)\n",
    "end\n",
    "\n",
    "function softmax(x::Array{Float64, 1})\n",
    "    ret = exp.(x)\n",
    "    ret / sum(ret)\n",
    "end\n",
    "\n",
    "function softmax(x::Array{Float32, 2}; dims=1)\n",
    "    mapslices(softmax, x; dims=dims)\n",
    "end\n",
    "\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \");readline()[1]=='y')\n",
    "        results = Float64[]; updates = 0; prog = ProgressMeter.Progress(60000)\n",
    "        function callback(J)\n",
    "            if updates % 600 == 0\n",
    "                push!(results, nll(model,dtrn), nll(model,dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "                ProgressMeter.update!(prog, updates)\n",
    "            end\n",
    "            return (updates += 1) <= 60000\n",
    "        end\n",
    "        train!(model, dtrn; callback=callback, optimizer=SGD(lr=0.1), o...)\n",
    "        Knet.save(file,\"results\",reshape(results, (4,:)))\n",
    "    end\n",
    "    results = Knet.load(file,\"results\")\n",
    "    println(minimum(results,dims=2))\n",
    "    return results\n",
    "end\n",
    "\n",
    "struct Linear; w; b; end\n",
    "(f::Linear)(x) = (f.w * x .+ f.b)\n",
    "\n",
    "Linear(inputsize::Int,outputsize::Int) = Linear(param(outputsize,inputsize),param0(outputsize))\n",
    "param(d...; init=xavier, atype=atype())=Param(atype(init(d...)))\n",
    "param0(d...; atype=atype())=param(d...; init=zeros, atype=atype)\n",
    "xavier(o,i) = (s = sqrt(2/(i+o)); 2s .* rand(o,i) .- s)\n",
    "atype()=(gpu() >= 0 ? KnetArray{Float32} : Array{Float32})\n",
    "\n",
    "struct MLP; layers; end\n",
    "MLP(h::Int...)=MLP(Linear.(h[1:end-1], h[2:end]))\n",
    "\n",
    "function (m::MLP)(x; pdrop=0, smax=false)\n",
    "    for (i,layer) in enumerate(m.layers)\n",
    "        p = (i <= length(pdrop) ? pdrop[i] : pdrop[end])\n",
    "        x = dropout(x, p)     # <-- This one line helps generalization\n",
    "        x = layer(x)\n",
    "        x = (layer == m.layers[end] ? x : relu.(x))\n",
    "    end\n",
    "    if smax\n",
    "        return softmax(x)\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end\n",
    "\n",
    "function accuracy(m)\n",
    "    correct = 0.0\n",
    "    wrong = 0.0\n",
    "    for (x,y) in dtst\n",
    "        pred = model(x; smax=true)\n",
    "        inds = argmax(pred, dims=1)\n",
    "        for (i, ind) in enumerate(inds)\n",
    "            if ind[1] == y[i]\n",
    "                correct += 1\n",
    "            else\n",
    "                wrong += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    correct / (correct + wrong)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on malware data\n",
    "Though the model is relatively simple, and the data set small, it acheives 98.8% accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37363636363636366\n",
      "Train from scratch? stdin> yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  21%|█████████                                |  ETA: 0:04:04\u001b[39m"
     ]
    }
   ],
   "source": [
    "model = MLP(215, 64, 2)\n",
    "println(accuracy(model))\n",
    "mlp = trainresults(\"mlp_trial.jld2\", model)\n",
    "println(accuracy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Adversarial Attempt\n",
    "\n",
    "Since the data for each field is a binary 0 or 1, I began by searching the space of all adjacent (differing only in 1 feature) possible feature vectors to find an adversarial example. This could be put into practice by adding or removing a single API call from the malware to cause this neural net to misclassify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function near_search(x0; model=model)\n",
    "    best_x = nothing\n",
    "    best_score = 0\n",
    "    for (i, x) in enumerate(x0)\n",
    "        if x == 0\n",
    "            new_x = x0\n",
    "            new_x[i] = 1 - x\n",
    "            pred = model(new_x, smax=true)\n",
    "            if pred[1] > pred[2] && pred[1] > best_score\n",
    "                best_x = new_x\n",
    "                best_score = pred[1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return best_x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = size(susp,2)\n",
    "successes = 0\n",
    "for i in 1:size(susp,2)\n",
    "    x0 = susp[:,i]\n",
    "    pred0 = model(x0, smax=true)\n",
    "    x1 = near_search(x0)  \n",
    "    pred = model(x1, smax=true)\n",
    "    if pred[1] > pred[2]\n",
    "        successes += 1\n",
    "    end\n",
    "end\n",
    "successes / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, for every single suspicious application, I found an adjacent application that the model will classify as benign. For some next steps, consider adding dropout to the trained model and see if that affects its susceptibility to this simple attack, and perhaps try defensive distillation, as documented in \"Distillation as a defense to adversarial perturbations against deep neural networks\" by Papernot et al (2016). To strengthen the attack, consider allowing larger perturbations or following gradients to find perturbation. Once the attack is sufficiently strong, I can try adversarial learning, where I train the model on the data, generate adversarial examples, then train it on the adversarial examples.\n",
    "\n",
    "For a parallel problem, consider adversarial examples for mnist data (code derived from the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  99%|█████████████████████████████████████████|  ETA: 0:00:01\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00609881; 0.0857389; 0.0005; 0.0248]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:01:36\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9749"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xsize=784\n",
    "ysize=10\n",
    "dtrn,dtst = mnistdata(xsize=(xsize,:))\n",
    "\n",
    "model = MLP(784,64,10)\n",
    "accuracy(model)\n",
    "mlp2 = trainresults(\"mlp2.jld2\", model);\n",
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0x07"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = []\n",
    "for (x,y) in dtst\n",
    "    if y != 1\n",
    "        push!(examples, (x,y))\n",
    "    end\n",
    "end\n",
    "examples[1][2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32[1.09696e-11, 1.49714e-6, 1.5021e-7, 6.9651e-12, 8.79741e-10, 1.82453e-18, 0.999997, 2.12942e-8, 1.47317e-6, 2.45452e-10]\n",
      "Starting optimization with optimizer DiffEvoOpt{FitPopulation{Float64},RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},RandomBound{RangePerDimSearchSpace}}\n",
      "0.00 secs, 0 evals, 0 steps\n",
      "\n",
      "Optimization stopped after 10001 steps and 0.29200005531311035 seconds\n",
      "Termination reason: Max number of steps (10000) reached\n",
      "Steps per second = 34249.99351207647\n",
      "Function evals per second = 34571.91125931527\n",
      "Improvements/step = 0.2182\n",
      "Total function evaluations = 10095\n",
      "\n",
      "\n",
      "Best candidate found: [0.821732, 0.269588, 0.557876, 0.466721, 0.725955, 0.303658, 0.341389, 0.375764, 0.338666, 0.244843, 0.512377, 0.363907, 0.468888, 0.270758, 0.596601, 0.322219, 0.366059, 0.377868, 0.639013, 0.385687, 0.674143, 0.520954, 0.489032, 0.510428, 0.401437, 0.381548, 0.354966, 0.410504, 0.496946, 0.333049, 0.417577, 0.132851, 0.374933, 0.361233, 0.263818, 0.124261, 0.147817, 0.58851, 0.58053, 0.265955, 0.689428, 0.306196, 0.141123, 0.113447, 0.296194, 0.255326, 0.397749, 0.499792, 0.498357, 0.291138, 0.421698, 0.625089, 0.346503, 0.300906, 0.315857, 0.197435, 0.473641, 0.232055, 0.459676, 0.0793344, 0.454411, 0.301004, 0.423774, 0.500127, 0.403167, 0.611472, 0.27805, 0.755188, 0.40293, 0.390573, 0.179623, 0.398473, 0.189493, 0.539296, 0.285415, 0.208336, 0.468709, 0.289587, 0.162116, 0.163396, 0.406684, 0.538573, 0.551495, 0.232876, 0.501907, 0.63912, 0.354193, 0.348764, 0.201417, 0.350114, 0.40925, 0.765695, 0.405848, 0.44271, 0.499674, 0.361786, 0.388138, 0.170518, 0.356039, 0.423732, 0.438846, 0.432893, 0.592037, 0.541434, 0.276732, 0.467846, 0.378511, 0.409519, 0.120683, 0.454846, 0.224651, 0.298603, 0.327797, 0.488727, 0.312493, 0.409205, 0.270482, 0.276438, 0.0954059, 0.532875, 0.389979, 0.29411, 0.360072, 0.420586, 0.49948, 0.762265, 0.692461, 0.626835, 0.459326, 0.393184, 0.296942, 0.242033, 0.623561, 0.613909, 0.293418, 0.142357, 0.180808, 0.608147, 0.261801, 0.544563, 0.317506, 0.205087, 0.411579, 0.253382, 0.406783, 0.664362, 0.398937, 0.381338, 0.345033, 0.294692, 0.482219, 0.424765, 0.537351, 0.502537, 0.177205, 0.589273, 0.467311, 0.566503, 0.574317, 0.407527, 0.287696, 0.586641, 0.525068, 0.278668, 0.351822, 0.396423, 0.489532, 0.435333, 0.341611, 0.35179, 0.282803, 0.178569, 0.53882, 0.374126, 0.239363, 0.325225, 0.302137, 0.191561, 0.521444, 0.234292, 0.440852, 0.42166, 0.119582, 0.305017, 0.202565, 0.18186, 0.42479, 0.18938, 0.404045, 0.413783, 0.286321, 0.166084, 0.337008, 0.392045, 0.492379, 0.0837617, 0.410051, 0.38061, 0.482076, 0.0976414, 0.19466, 0.506216, 0.623195, 0.680852, 0.421551, 0.465122, 0.378099, 0.393309, 0.311279, 0.439695, 0.295035, 0.25969, 0.425975, 0.43833, 0.372127, 0.36709, 0.505381, 0.368538, 0.404958, 0.0425443, 0.42084, 0.33871, 0.398949, 0.567429, 0.367075, 0.38419, 0.427244, 0.471976, 0.311745, 0.487253, 0.543349, 0.281379, 0.473197, 0.515389, 0.21721, 0.77801, 0.614892, 0.56728, 0.467812, 0.63242, 0.397235, 0.437343, 0.32507, 0.406338, 0.49776, 0.375124, 0.321915, 0.226147, 0.316282, 0.460331, 0.124108, 0.277411, 0.349504, 0.604853, 0.278353, 0.427445, 0.413555, 0.0938841, 0.521798, 0.435002, 0.801953, 0.460075, 0.689922, 0.666438, 0.765336, 0.437479, 0.192869, 0.732412, 0.600756, 0.341349, 0.791423, 0.385264, 0.538254, 0.581946, 0.507903, 0.364091, 0.422672, 0.410829, 0.272327, 0.183175, 0.242362, 0.0815672, 0.359999, 0.245488, 0.509906, 0.359473, 0.618301, 0.375985, 0.343744, 0.228683, 0.120804, 0.582694, 0.280729, 0.445889, 0.109579, 0.600523, 0.448186, 0.434072, 0.613535, 0.459757, 0.699244, 0.376484, 0.347598, 0.298698, 0.311294, 0.479771, 0.310091, 0.370277, 0.437636, 0.674273, 0.492554, 0.648255, 0.424066, 0.454138, 0.478166, 0.396757, 0.320788, 0.350623, 0.362877, 0.488449, 0.269874, 0.0892103, 0.309651, 0.474022, 0.631094, 0.512276, 0.586253, 0.695938, 0.671684, 0.315945, 0.235948, 0.316645, 0.227364, 0.354217, 0.331842, 0.485254, 0.51845, 0.384256, 0.384682, 0.38508, 0.327651, 0.439951, 0.197975, 0.188631, 0.24799, 0.39087, 0.309961, 0.289228, 0.339124, 0.306037, 0.513165, 0.322688, 0.374437, 0.46095, 0.419293, 0.69364, 0.175208, 0.761024, 0.688148, 0.306451, 0.625127, 0.253009, 0.475208, 0.32952, 0.521742, 0.571092, 0.146727, 0.338136, 0.426166, 0.352125, 0.374541, 0.445401, 0.559929, 0.461722, 0.280206, 0.387071, 0.227206, 0.170341, 0.208734, 0.431887, 0.347208, 0.597066, 0.558343, 0.51664, 0.468547, 0.0732379, 0.619424, 0.444269, 0.434655, 0.409516, 0.212337, 0.283737, 0.538542, 0.567697, 0.33658, 0.657664, 0.563237, 0.259375, 0.117741, 0.380744, 0.44023, 0.334416, 0.315722, 0.378848, 0.260691, 0.143742, 0.48515, 0.22722, 0.554318, 0.639503, 0.652737, 0.721045, 0.28649, 0.695563, 0.397299, 0.568416, 0.633134, 0.495844, 0.375627, 0.568257, 0.269722, 0.45111, 0.447074, 0.300403, 0.489796, 0.379518, 0.442333, 0.520587, 0.11849, 0.374091, 0.454227, 0.341313, 0.499551, 0.441835, 0.334752, 0.338297, 0.406633, 0.698089, 0.564694, 0.453978, 0.406126, 0.374265, 0.668874, 0.409916, 0.532331, 0.698359, 0.412304, 0.223237, 0.690808, 0.359806, 0.223784, 0.585499, 0.521395, 0.114307, 0.298716, 0.481435, 0.349195, 0.624863, 0.310168, 0.273197, 0.189273, 0.174245, 0.436556, 0.400301, 0.50518, 0.623207, 0.771977, 0.266653, 0.519803, 0.506868, 0.417102, 0.414094, 0.324126, 0.627274, 0.203144, 0.404767, 0.433572, 0.3862, 0.351908, 0.604339, 0.802323, 0.573337, 0.419431, 0.456663, 0.311993, 0.389532, 0.589054, 0.712977, 0.312552, 0.250444, 0.712174, 0.651475, 0.257861, 0.180396, 0.532603, 0.240669, 0.505626, 0.693036, 0.325257, 0.393117, 0.425878, 0.645787, 0.499386, 0.37159, 0.296652, 0.565719, 0.210921, 0.522993, 0.310907, 0.155879, 0.557058, 0.223871, 0.595342, 0.489449, 0.244564, 0.274338, 0.280354, 0.443122, 0.539562, 0.392788, 0.941417, 0.441173, 0.501641, 0.190102, 0.262106, 0.345239, 0.443584, 0.294058, 0.599011, 0.604063, 0.413682, 0.125304, 0.731685, 0.290824, 0.321088, 0.717099, 0.396879, 0.406796, 0.527509, 0.300649, 0.558592, 0.387192, 0.401824, 0.481017, 0.130306, 0.136948, 0.628012, 0.926015, 0.495746, 0.625156, 0.238745, 0.296686, 0.450795, 0.54082, 0.155347, 0.520105, 0.510448, 0.58703, 0.502644, 0.508617, 0.375728, 0.506858, 0.222424, 0.471024, 0.345626, 0.316047, 0.268698, 0.677275, 0.64249, 0.540249, 0.490553, 0.328172, 0.210882, 0.554383, 0.595577, 0.801514, 0.143577, 0.691663, 0.527593, 0.249699, 0.399599, 0.450914, 0.268323, 0.398342, 0.330122, 0.503199, 0.60513, 0.42509, 0.212541, 0.499164, 0.323357, 0.351632, 0.163751, 0.487227, 0.372843, 0.358666, 0.241939, 0.740558, 0.815107, 0.206466, 0.382983, 0.506451, 0.818236, 0.579241, 0.505429, 0.532537, 0.555002, 0.424169, 0.338046, 0.388097, 0.481051, 0.411835, 0.41696, 0.494836, 0.57594, 0.218464, 0.598181, 0.230998, 0.364357, 0.277378, 0.391304, 0.743492, 0.395668, 0.581687, 0.221493, 0.391274, 0.283244, 0.623298, 0.404149, 0.619046, 0.383981, 0.342282, 0.321116, 0.534922, 0.324396, 0.547611, 0.575108, 0.259365, 0.309865, 0.457348, 0.453821, 0.691103, 0.377552, 0.513031, 0.416849, 0.273468, 0.270301, 0.326607, 0.35667, 0.282526, 0.577389, 0.267205, 0.260957, 0.488377, 0.400528, 0.314804, 0.623102, 0.522027, 0.292284, 0.399224, 0.477355, 0.451247, 0.153188, 0.389518, 0.565206, 0.59571, 0.14655, 0.346809, 0.189022, 0.229924, 0.607748, 0.304962, 0.536083, 0.34769, 0.543923, 0.621465, 0.481894, 0.441868, 0.357418, 0.401148, 0.362862, 0.728466, 0.39693, 0.616559, 0.419864, 0.826991, 0.593495, 0.199617, 0.209163, 0.351182, 0.152726, 0.584137, 0.389869, 0.18772, 0.284086, 0.39351, 0.371522, 0.625777, 0.199199, 0.252979, 0.537853, 0.25148, 0.546303, 0.361107, 0.530064, 0.281061, 0.523488, 0.381387, 0.566459, 0.247541, 0.552084, 0.390441, 0.697836, 0.303118, 0.455387, 0.534534, 0.332147, 0.117668, 0.416999, 0.291535, 0.287729, 0.26864, 0.600265, 0.493989, 0.413306, 0.451455, 0.171681, 0.431753, 0.559017, 0.408152, 0.449176, 0.511104, 0.536788, 0.321949, 0.516303, 0.491626, 0.378316, 0.43694, 0.305907, 0.541902, 0.557398, 0.306512, 0.445309, 0.426968, 0.667931, 0.0931148, 0.530938, 0.200859, 0.448658, 0.629929, 0.508988, 0.431334, 0.121633, 0.33572, 0.272243, 0.334517, 0.32793, 0.431124, 0.390915, 0.487401, 0.354974, 0.275689, 0.52597, 0.215206, 0.258436, 0.265728, 0.456386, 0.528709, 0.174604, 0.0972915, 0.431742, 0.638628, 0.457013, 0.417832, 0.264114, 0.369968, 0.60012, 0.447605, 0.460538, 0.321464, 0.470473, 0.375351, 0.320223, 0.479869]\n",
      "\n",
      "Fitness: 133.987060127\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49008, 0.115383, 0.0920092, 3.29477e-5, 0.0628615, 0.000633907, 1.35619e-5, 0.120905, 0.00306327, 0.115017]\n",
      "11.544430411038785\n"
     ]
    }
   ],
   "source": [
    "using Distances\n",
    "using BlackBoxOptim\n",
    "\n",
    "# This x0 is supposed to be a 7\n",
    "x0 = examples[1][1][:, 1]\n",
    "\n",
    "function error(xp; model=model, c=1, x=x0, target=1)\n",
    "    \"\"\"\n",
    "    Computes a value to be minimized when generating adversarial examples\n",
    "    model is the model to fool\n",
    "    c is a parameter to tun how much to weigh distance from the original\n",
    "    x0 is the original\n",
    "    target is the target class\n",
    "    \n",
    "    See \"Intriguing properries of neural networks\" Szegedy et al (2013)\n",
    "    \"\"\"\n",
    "    c * euclidean(x, xp)^2 - log(model(xp, smax=true)[target])\n",
    "end\n",
    "\n",
    "println(model(x0, smax=true))\n",
    "x1 = bboptimize(error; SearchRange = (0.0, 1.0), NumDimensions = 784).archive_output.best_candidate\n",
    "println(model(x1, smax=true))\n",
    "println(euclidean(x1, x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adversarial noise was added, the model (correctly) predicted that the input was a 7 with 99.9% confidence. After perturbation, it is most confident (49%) that the input is a 1, despite the two being only 11.5 apart in terms of Euclidean distance.  \n",
    "This method can be refined through use of a line-search to find the value of c that minimizes the distance of the adversarial example from the original while still fooling the model.  \n",
    "(I would also like to find a way to disable all that output from bboptimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function binary_search_old(f; max_depth=5, max_x=100)\n",
    "    \"\"\"\n",
    "    Uses a binary search to minimize f\n",
    "    Needs some updating. If it hasn't hit the target, search down\n",
    "    Otherwise search up\n",
    "    \"\"\"\n",
    "    curr_x = max_x / 4\n",
    "    curr_diff = max_x / 2\n",
    "    cont = true\n",
    "    for i in 1:max_depth\n",
    "        val_l = f(curr_x)\n",
    "        val_r = f(curr_x + curr_diff)\n",
    "        curr_x = val_l < val_r ? curr_x - curr_diff / 4 : curr_x + 3 * curr_diff / 4\n",
    "        curr_diff = curr_diff / 2\n",
    "    end\n",
    "    return curr_x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~': ASCII/Unicode U+007e (category Sm: Symbol, math)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"龙abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
